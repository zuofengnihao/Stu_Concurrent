# Stu_Concurrent

## 第1章 并发编程的挑战

### 1.1 上下文切换
当前线程要切换到另外一个线程的时候，需要保存当先线程的状态（如：寄存器里面已经计算好了的值，当前运行到哪段代码了等信息），然后读取或叫加载另外那个线程的状态，这就是上下文的切换。

#### 1.1.1 多线程一定快吗
不一定，因为线程有创建和上下文切换的开销。

#### 1.1.2 测试上下文切换次数和时长
* 使用Lmbench可测量上下文切换带来的消耗
* 使用vmstat可测量上下文切换的次数

#### 1.1.3 如何减少上下文的切换
1. 使用无锁并发编程：多线程竞争锁，会引起上下文的切换。可以使用一些办法避免使用锁。如：将数据ID按照Hash算法取模来分段，不同线程处理不同段的数据。
2. CAS算法：Atomic包使用CAS算法，不需要加锁。
3. 使用最少的线程：避免创建不需要的线程。
4. 协程：我理解的协程就是在用户态使用代码实现的多线程，和内核态的线程是一对多的关系。协程不需要切换到内核态来完成任务的调度与切换。当然协程也是需要记录和切换上下文的，只是在用户态中，程序员是自己可以用代码来压榨切换带来的最小开销极限。

#### 1.1.4 减少上下文切换实战
* 第一步：用jstack命令dump线程信息。
* 第二步：统计线程状态。
* 第三步：打开dump文件查看处于WAITING的线程在做什么。
* 第四步：减少无效的WAITING状态的线程数量。

### 1.2 死锁
避免死锁的常见方法：
1. 避免一个线程同时获取多个锁。
2. 避免一个线程在锁内同时占用多个资源，尽量保证每个锁只占用一个资源。
3. 尝试使用定时锁，lock.tryLock(timeout)。
4. 对于数据库锁，加锁和解锁必须在一个数据库链接里。

### 1.3 资源限制的挑战

1. 什么是资源限制  
2. 资源限制引发的问题
3. 如何解决资源限制
4. 在资源限制情况下进行并发

### 1.4 本章小结

## 第2章 Java并发机制的底层实现原理

Java中所使用的并发机制依赖于JVM的实现和CPU的指令。

### 2.1 Volatile的应用

在多处理器开发中保证了共享变量的“可见性”。

#### 2.1.1 Volatile的定义与实现原理

定义：Java编程语言允许线程访问共享变量，为了确保共享变量能被准确和一直的更新，线程应该确保通过排它锁单独获得这个变量。  

CPU的相关术语：
* 内存屏障（Memory barriers）：是一组处理器指令，用于实现对内存操作的顺序限制。
* 缓冲行（cache line）：缓存中可分配的最小存储单位。处理器填写缓存线时会加载整个缓存线，需要使用多个主内存读周期。
* 原子操作（atomic operations）：不可中断的一个或一系列操作。
* 缓存行填充（cache line fill）：当处理器识别到从内存中读取操作数是可缓存的，处理器读取整个缓存行到适当的缓存（L1/L2/L3）。
* 缓存命中（cache hit）：如果进行高速缓存行填充操作的内存位置仍然是下次处理器访问的地址时，处理器从缓存中读取操作数，而不是从内存中。
* 写命中（write hit）：当处理器将操作数写回到一个内存缓存的区域时，它会首先检查这个缓存的内存地址是否在缓存行中，如果存在一个有效的缓存行，则处理器将这个操作数写回到缓存，而不是写回到内存，这个操作被称为写命中。
* 写缺失（write misses the cache）：一个有效的缓存行被写入到不存在的内存区域。  

原理：如果对声明了volatile的变量进行写操作，JVM就会向处理器发送一条Lock前缀的指令，将这个变量所在缓存行的数据写回到系统内存。通过实现缓存一致性协议（MESI）每个处理器通过嗅探在总线上传播的数据来检查自己缓存的值是不是过期了，当处理器发现自己缓存行对应的内存地址被修改，就会将当前处理器的缓存行设置成无效状态，当处理器对这个数据进行修改操作的时候，会重新从系统内存中把数据读到处理器缓存里。

#### 2.1.2 Volatile的使用优化

避免伪共享（false sharing）：Doug Lea 使用追加字节的方式来优化。

### 2.2 Synchronized的实现原理与应用

Synchronized同步代码块是使用monitorenter和monitorexit指令实现的。同步方法是另一种方式实现。

锁对象的三种形式：
1. 普通同步方法，锁是当前的实例对象。
2. 静态同步方法，锁是当前类的Class对象。
3. 同步代码块，锁是Synchronized括号中里配置的对象。

#### 2.2.1 Java对象头

* Mark Word 32/64bit 存储对象的hashCode或锁信息等  
* Class Metadata Address 32/64bit 存储到对象类型数据的指针  
* Array Length 32/32bit 数组长度（如果当前对象是数组）  

64位虚拟机 Mark Word：
* 1bit 是否偏向锁 
* 2bit 锁标志位
* 4bit GC分代年龄
* 31bit hashcode    
* 25bit unused

#### 2.2.2 锁定升级与对比

1. 偏向锁：  
   进入同步代码块，检查锁对象头markword是否有锁（锁标志位或是否偏向锁）。  
   * 如果是无锁状态  
     * 检查是否开启偏向锁  
       * 开启，cas修改markword偏向当前线程
          * 修改成功，执行同步代码块
          * 修改失败，先暂停偏向的线程，检查线程是否退出同步代码块
            * 退出，撤销偏向锁，唤醒原持有锁的线程
            * 未退出，升级为轻量级锁
       * 未开启，轻量级锁流程...
   * 有锁状态，是否为偏向锁
     * 是偏向锁，检查偏向线程是否为当前线程
       * 是，执行同步代码块
       * 否，cas修改markword偏向当前线程
         * 修改成功，执行同步代码块
         * 修改失败，先暂停偏向的线程，检查线程是否退出同步代码块 
           * 退出，撤销偏向锁，唤醒原持有锁的线程
           * 未退出，升级为轻量级锁
      * 否，按照当前锁的方法来执行 

2. 轻量级锁
   进入同步代码块，检查是否为偏向锁升级
   * 是偏向是升级，为偏向线程的栈中分配锁记录
   * 不是偏向锁升级，为当前线程的栈中分配所记录  
   
   CAS操作，将对象头中的markword复制到锁记录中，displaced mark word
   * 操作成功，markword指向当前线程的锁记录中，执行同步代码块，执行完毕后cas修改markword（改回）
     * 成功，解锁完成
     * 失败，升级重量级锁
   * 操作失败，自旋等待
     * 多次自选失败，升级重量级锁
       
3. 锁的优缺点  
   
   偏向锁：适用于只有一个线程访问的场景。优点是加锁一次cas操作，缺点是如果线程参在额外竞争会带来额外的撤销锁的消耗。  
   
   轻量级锁：适用于同步块执行速度快的场景。优点是竞争的线程不会阻塞，缺点是cup自旋消耗。  
   
   重量级锁：适用于追求吞吐量，同步代码块执行长。优点不会消耗cpu，缺点就是阻塞线程，响应慢。
   
### 2.3 原子操作的实现原理

1. 处理器如何实现原子操作
   * 使用总线加锁
   * 使用缓存加锁

2. Java如何实现原子操作
   * 使用循环CAS实现（自旋锁）  
     Cas操作的三大问题
     1. ABA问题，解决：在参数中加入版本号
     2. 循环时间长开销大
     3. 只能保证一个共享变量的原子操作，解决：同步块 
   * 使用锁机制实现原子操作：偏向锁，轻量级锁，重量级锁。JVM实现锁的方式都用到了CAS操作，即获取锁和释放锁时。

### 2.4 本章小结

## 第3章 Java内存模型

### 3.1 Java内存模型的基础

#### 3.1.1 并非编程模型的两个关键

线程之间如何通信及线程间如何同步？  

线程间的通讯有两种机制：
1. 共享内存：共享内存的线程间通讯是隐式的，同步则是显式的。
2. 消息传递：消息传递的线程间通讯是显示的，同步则是隐式的。  

Java的并发采用的是共享内存的方式。

#### 3.1.2 Java内存模型的抽象结构

线程之间的共享变量存储在主存中，每个线程都有一个私有的本地内存，本地内存中存储了该线程以读/写的共享变量的副本。本地内存是JMM的一个抽象概念，它涵盖了缓存、写缓冲区、寄存器以及其他的硬件和编译优化。  

如果线程A与线程B之间要通讯的话，必须要经历2个步骤：
1. 线程A把本地内存A中更新过的共享变量刷新到主存中。
2. 线程B到主存中读取线程A之前更新变量。  

从整体上看，这两个步骤实质上是线程A向线程B发送消息，而这个通信过程必须要经过主存。JMM通过控制主存与每个线程的本地内存之间的交互来为JAVA程序员提供内存可见性保证。

#### 3.1.3 从源代码到指令序列的重排序

为了提高性能，编译器和处理器常常会对指令做重排序。分为3重类型：
1. 编译器优化的重排序。
2. 指令级并行重排序。
3. 内存系统的重排序。

1属于编译器重排序，2和3属于处理器重排序。  
JMM的编译器重排序规则会禁止特定类型的编译器重排序。而对于处理器重排序，会在生成指令序列时，插入特定类型的内存屏障来禁止。

#### 3.1.4 并发编程模型的分类

现代处理器使用写缓冲区临时存放向内存写入的数据。每个写缓冲区只对该核心可见，这个特性会对内存操作顺序产生重要的影响！  
例：int x = 0 在主存中，线程A执行 x = 1，然后线程B执行 print(x),打印结果可能会为“0”，因为线程A写入数据时写入的是缓存区，而线程B读取的是主存中的数据。在我们看来，像是线程B先于线程A执行了，即写读变成了读写。因此，大部分处理器都支持写读操作的重排序。

为了保证可见性，Java编译器在生成指令序列的适当位置会插入内存屏障指令来禁止特定类型的处理器的重排序。JMM把内存屏障分为4类。
1. LoadLoad Barriers `Load1;LoadLoad;Load2`：确保Load1数据的装载先于load2及后续装在指令的装在
2. StoreStore Barriers `Store1;StoreStore;Store2`：确保Store1数据对其他处理器可见（刷新回主存）先于Store2
3. LoadStore Barriers `Load1;LoadStore;Store2`：确保Load1数据装载先于Store2
4. StoreLoad Barriers `Store1;StoreLoad;Load2`：确保Store1数据对其他线程可见，先于Load2。该屏障之前的所有内存访问指令完成后，才执行该屏障之后的内存访问指令。StoreLoad Barriers是一个全能型的屏障，它同时具有以上3个屏障的效果。执行该屏障开销会很昂贵，因为当前处理器通常要把写缓冲区中的数据全部刷新到内存中。

#### 3.1.5 happens-before简介

从JDK5开始，Java使用新的JSR-133内存模型。JSR-133使用happens-before的概念来阐述操作之间的内存可见性。如果一个操作执行的结果需要另一个操作可见，那么这两个操作之间必须存在happens-before关系。
* 程序顺序规则：一个线程中的每个操作，happens-before于该线程中的任意后续操作。
* 监视器锁规则：对一个锁的解锁，happens-before于随后对这个锁的加锁。
* volatile变量规则：对一个volatile域的写，happens-before于任意后续对这个volatile域的读。
* 传递性：如果A happens-before B，B happens-before C，则A happens-before C。

注意：两个操作之间具有happens-before原则不代表前一个操作必须在后一个操作之前执行！happens-before仅仅要求前前一个操作执行结果对后一个操作可见，且前一个操作按顺序排在第二个操作之前。

对于Java程序员来说，happens-before规则简单，它避免了我们为了理解JMM提供的内存可见性保证而去学习复杂的重排序规则以及这些规则的具体实现方法。

### 3.2 重排序

重排序是指编译器和处理器为了优化程序性能而对指令序列重新排序的一种手段。

#### 3.2.1 数据依赖性

如果两个操作访问同一个变量，且这两个操作中有一个为写操作，此时两个操作之间就存在数据依赖性。数据依赖性分为一下3种
1. 写后读 `a=1;b=a;` 写一个变量之后，再读这个变量。
2. 写后写 `a=1;a=2;` 写一个变量之后，再写这个变量。
3. 读后写 `a=b;a=1;` 读一个变量之后，再写这个变量。

以上3种操作，只要重排序两个操作的顺序，程序的执行结果就会改变。  
这里所说的数据依赖性仅针对单个处理器中执行的指令序列和单个线程中执行的操作。不同处理器线程之间的数据依赖性不被编译器和处理器考虑。

#### 3.2.2 as-if-serial语义

意思：不管怎么重排序，单线程程序的执行结果不能改变。编译器、runtime和处理器都必须遵守as-if-serial。  
as-if-serial语义把单线程程序保护了起来，使程序员产生了幻觉：单线程程序是按程序的顺序来执行的。

#### 3.2.3 程序顺序规则

在不改变程序执行结果的前提下，尽可能提高并行度。编译器和处理器遵从这一目标，从happens- before的定义我们可以看出，JMM同样遵从这一目标。  

#### 3.2.4 重排序对多线程的影响

1. 重排序会破坏多线程的语义。
2. 在单线程中，对存在控制依赖的操作重排序不会改变执行结果。但在多线程中，存在控制依赖的操作重排序，可能会改变程序的执行结果。

### 3.3 顺序一致性

顺序一致性内存模型是一个理论参考模型，处理器的内存模型和编程语言的内存模型都会以顺序一致性内存模型作为参照。

#### 3.3.1 数据竞争与数据一致性

如果程序是正确同步的，程序的执行将具有顺序一致性。

#### 3.3.2 顺序一致性模型

两大特性：
1. 一个线程中所有操作必须按照程序的顺序执行。
2. （不管程序是否同步）所有线程都只能看到一个单一的操作执行顺序，每个操作都必须原子执行且立刻对所有线程可见。

#### 3.3.3 同步程序的顺序一致性效果

这是一个正确同步的多线程程序。根据JMM规范，该程序的执行结果将与该程序在顺序一致性模型中的执行结果相同。  
JMM在具体实现上的基本方针为：在不改变（正确同步的）程序执行结果的前提下，尽可能地为编译器和处理器的优化打开方便之门。

#### 3.3.4 未同步程序的执行特性

JMM只提供最小安全性：线程执行时读取到的值，要么就是之前某个线程写入的，要么是默认值。  

JMM与顺序一致性的差异：
1. 顺序一致性保证单线程内的操作会按程序的顺序执行，JMM不能保证。
2. 顺序一致性模型保证所有线程只能看到一致的操作执行顺序，JMM不能保证。
3. JMM不保证对64位数据的写操作具有原子性，顺序一致性保证所有的内存读/写都是原子的。

### 3.4 volatile的内存语义

#### 3.4.1 volatile的特性

* 可见性：对一个volatile变量的读，总是能看到对这个volatile变量最后的写入。
* 原子性：对任意单个volatile变量的读/写具有原子性， 但类似++这种复合操作不具备原子性。

#### 3.4.2 volatile写-读建立的happens-before关系

从JSR-133开始，volatile变量的读写可以实现线程之间的通信。  
volatile的写-读，与锁的释放-获取有相同的内存效果。

#### 3.4.3 volatile写-读的内存语义

* 线程A写一个volatile变量，实质上是线程A向接下来将要读这个volatile变量的某个线程发出了消息。
* 线程B读一个volatile变量，实质上是线程B接收了之前某个线程发出的消息。
* 线程A写一个volatile变量，随后线程B读这个变量，实质上是线程A通过主存向B发送消息。

#### 3.4.4 volatile内存语义的实现

* 当第二个操作是volatile写时，不管第一个操作是什么，都不能重排序。这个规则保证volatile写之前的操作不会被编译器重排序到volatile写之后。
* 当第一个操作是volatile读时，不管第二个操作是什么，都不能重排序到volatile读之前。
* 当第一个操作是volatile写，第二个操作是volatile读时，不能重排序。

内存屏障：
* 在每个volatile写操作之前插入一个StoreStore屏障。
* 在每个volatile写操作之后插入一个StoreLoad。
* 在每个volatile读操作之后插入一个LoadLoad屏障。
* 在每个volatile读操作之前插入一个LoadStore。

在X86处理器中，仅会对写-读操作做重排序。因此JMM仅需在volatile写后面插入一个StoreLoad屏障。

#### 3.4.5 JSR-133为什么增强volatile内存语义

因为volatile的写-读没有锁的释放-获取具有的内存语义。

### 3.5 锁的内存语义

#### 3.5.1 锁的释放-获取建立的happens-before关系
与volatile写-读相似

#### 3.5.2 锁的释放和获取的内存语义
与volatile写-读相似 实际上是线程A向线程B发送消息。

#### 3.5.3 锁内存语义的实现

以ReentrantLock的源码为例
公平锁的加锁（获取锁）是读取一个volatile变量（state），公平锁的释放锁是写volatile变量state。
非公平锁的加锁（获取锁）是CAS操修改state变量，非公平锁的释放也是volatile变量的写操作。

CAS如何同时具有volatile读和写的内存语义的


